import requests
import re
from bs4 import BeautifulSoup
import ast
from functools import lru_cache

# Define the cache size and expiration time
CACHE_SIZE = 128
CACHE_EXPIRATION = 3600  # 1 hour in seconds
SENSITIVE_DATA_ATTRIBUTES = ['src', 'href', 'data-src']

def get_page_content(url):
    try:
        response = requests.get(url)
        response.raise_for_status()  # Raise an HTTPError for bad responses
        return response.text
    except requests.RequestException as e:
        raise Exception(f"Error fetching content from {url}: {str(e)}")



def extract_javascript_links(url):
    try:
        # Use the cached content if available
        page_content = get_page_content(url)
        soup = BeautifulSoup(page_content, 'html.parser')

        javascript_links = []
        for script_tag in soup.find_all('script'):
            if 'src' in script_tag.attrs:
                javascript_links.append(script_tag['src'])

        return javascript_links

    except Exception as e:
        return f"Error extracting JavaScript links: {str(e)}"

@lru_cache(maxsize=CACHE_SIZE)

def analyze_javascript_files(urls):
    # Create an empty list to store sensitive data
    sensitive_data = []


    # Load spaCy model
    nlp = spacy.load("en_core_web_sm")

    # Loop through each URL and analyze its JavaScript file
    for url in urls:
        try:
            # Use requests to get the content of the JavaScript file
            response = requests.get(url)
            js_code = response.text

            # Analyze JavaScript code using spaCy
            doc = nlp(js_code)

            # Extract sensitive information based on linguistic analysis
            found_data = {}
            for ent in doc.ents:
                if ent.label_ == 'PERSON':
                    found_data.setdefault('Names', []).append(ent.text)
                elif ent.label_ == 'ORG':
                    found_data.setdefault('Organizations', []).append(ent.text)
                elif ent.label_ == 'DATE':
                    found_data.setdefault('Dates', []).append(ent.text)

    # Define regex patterns for sensitive data
    patterns = {
    'URLs': r'https?:\/\/(www\.)?[-a-zA-Z0-9@:%._\+~#=]{1,256}\.[a-zA-Z0-9()]{1,6}\b([-a-zA-Z0-9()@:%_\+.~#?&//=]*)',
    'Artifactory API Token': r'(?:\s|=|:|"|^)AKC[a-zA-Z0-9]{10,}',
    'Artifactory Password': r'(?:\s|=|:|"|^)AP[\dABCDEF][a-zA-Z0-9]{8,}',
    'Authorization Basic': r'basic [a-zA-Z0-9_\\-:\\.=]+',
    'Authorization Bearer': r'bearer [a-zA-Z0-9_\\-\\.=]+',
    'AWS Client ID': r'(A3T[A-Z0-9]|AKIA|AGPA|AIDA|AROA|AIPA|ANPA|ANVA|ASIA)[A-Z0-9]{16}',
    'AWS MWS Key': r'amzn\.mws\.[0-9a-f]{8}-[0-9a-f]{4}-[0-9a-f]{4}-[0-9a-f]{4}-[0-9a-f]{12}',
    'AWS Secret Key': r'(?i)aws(.{0,20})?(?-i)[\'\"][0-9a-zA-Z\/+]{40}[\'\"]',
    'Base32': r'(?:[A-Z2-7]{8})*(?:[A-Z2-7]{2}={6}|[A-Z2-7]{4}={4}|[A-Z2-7]{5}={3}|[A-Z2-7]{7}=)?',
    'Base64': r'(eyJ|YTo|Tzo|PD[89]|aHR0cHM6L|aHR0cDo|rO0)[a-zA-Z0-9+/]+={0,2}',
    'Basic Auth Credentials': r'(?<=:\/\/)[a-zA-Z0-9]+:[a-zA-Z0-9]+@[a-zA-Z0-9]+\.[a-zA-Z]+',
    'Cloudinary Basic Auth': r'cloudinary:\/\/[0-9]{15}:[0-9A-Za-z]+@[a-z]+',
    'Facebook Access Token': r'EAACEdEose0cBA[0-9A-Za-z]+',
    'Facebook Client ID': r'(?i)(facebook|fb)(.{0,20})?[\'\"][0-9]{13,17}[\'\"]',
    'Facebook Oauth': r'[f|F][a|A][c|C][e|E][b|B][o|O][o|O][k|K].*[\'|\"][0-9a-f]{32}[\'|\"]',
    'Facebook Secret Key': r'(?i)(facebook|fb)(.{0,20})?(?-i)[\'\"][0-9a-f]{32}',
    'Github': r'(?i)github(.{0,20})?(?-i)[\'\"][0-9a-zA-Z]{35,40}[\'\"]',
    'Google API Key': r'AIza[0-9A-Za-z\\-_]{35}',
    'Google Cloud Platform API Key': r'(?i)(google|gcp|youtube|drive|yt)(.{0,20})?[\'\"][AIza[0-9a-z\\-_]{35}[\'\"]',
    'Google Drive API Key': r'AIza[0-9A-Za-z\\-_]{35}',
    'Google Drive Oauth': r'[0-9]+-[0-9A-Za-z_]{32}\.apps\.googleusercontent\.com',
    'Google Gmail API Key': r'AIza[0-9A-Za-z\\-_]{35}',
    'Google Gmail Oauth': r'[0-9]+-[0-9A-Za-z_]{32}\.apps\.googleusercontent\.com',
    'Google Oauth Access Token': r'ya29\\.[0-9A-Za-z\\-_]+',
    'Google Youtube API Key': r'AIza[0-9A-Za-z\\-_]{35}',
    'Google Youtube Oauth': r'[0-9]+-[0-9A-Za-z_]{32}\.apps\.googleusercontent\\.com',
    'Heroku API Key': r'[h|H][e|E][r|R][o|O][k|K][u|U].{0,30}[0-9A-F]{8}-[0-9A-F]{4}-[0-9A-F]{4}-[0-9A-F]{4}-[0-9A-F]{12}',
    'IPv4': r'\b(25[0-5]|2[0-4][0-9]|[01]?[0-9][0-9]?)(\.(25[0-5]|2[0-4][0-9]|[01]?[0-9][0-9]?)){3}\b',
    'IPv6': r'((([0-9A-Fa-f]{1,4}:){1,7}[0-9A-Fa-f]{1,4})|(::([0-9A-Fa-f]{1,4}:){0,6}([0-9A-Fa-f]{1,4}:){1,6}|(([0-9A-Fa-f]{1,4}:){1,6}:)|([0-9A-Fa-f]{1,4}:){7,7}))',
    'Javascript Variables': r'(?:const|let|var)\s+\K(\w+?)(?=[;.=\s])',
    'LinkedIn Client ID': r'(?i)linkedin(.{0,20})?(?-i)[\'\"][0-9a-z]{12}[\'\"]',
    'LinkedIn Secret Key': r'(?i)linkedin(.{0,20})?[\'\"][0-9a-z]{16}[\'\"]',
    'Mailchamp API Key': r'[0-9a-f]{32}-us[0-9]{1,2}',
    'Mailgun API Key': r'key-[0-9a-zA-Z]{32}',
    'Mailto': r'(?<=mailto:)[a-zA-Z0-9_.+-]+@[a-zA-Z0-9-]+\.[a-zA-Z0-9.-]+',
    'MD5 Hash': r'[a-f0-9]{32}',
    'Picatic API Key': r'sk_live_[0-9a-z]{32}',
    'Slack Token': r'xox[baprs]-([0-9a-zA-Z]{10,48})?',
    'Slack Webhook': r'https://hooks.slack.com/services/T[a-zA-Z0-9_]{10}/B[a-zA-Z0-9_]{10}/[a-zA-Z0-9_]{24}',
    'Stripe API Key': r'(pk|sk|rk)_(test|live)_[A-Za-z0-9]+',
    'Square Access Token': r'sqOatp-[0-9A-Za-z\\-_]{22}',
    'Square Oauth Secret': r'sq0csp-[ 0-9A-Za-z\\-_]{43}',
    'Twilio API Key': r'SK[0-9a-fA-F]{32}',
    'Twitter Client ID': r'(?i)twitter(.{0,20})?[\'\"][0-9a-z]{18,25}',
    'Twitter Oauth': r'[t|T][w|W][i|I][t|T][t|T][e|E][r|R].{0,30}[\'\"\\s][0-9a-zA-Z]{35,44}[\'\"\\s]',
    'Twitter Secret Key': r'(?i)twitter(.{0,20})?[\'\"][0-9a-z]{35,44}',
    'Vault Token': r'[sb]\.[a-zA-Z0-9]{24}',
    'URL Parameter': r'(?<=\?|\&)[a-zA-Z0-9_]+(?=\=)',
    'Slack API Token': r'xox[pbrs]-[0-9]{12}-[0-9]{12}-[0-9a-zA-Z]{24}',
    'Slack OAuth Token': r'[s|S]LACK\s*[a-zA-Z0-9-]+',
    'Bitbucket OAuth Token': r'[b|B]itbucket\s*[a-zA-Z0-9]+',
    'Trello API Key': r'[t|T]rello\s*[a-zA-Z0-9]+',
    'Reddit Client ID': r'[r|R]eddit\s*([0-9a-zA-Z\-_]{14})',
    'Asana Personal Access Token': r'[a|A]sana\s*[0-9]{12}:[0-9a-zA-Z_-]+',
    'Zoom API Key': r'[z|Z]oom\s*[0-9]+\.[0-9a-zA-Z_-]+',
    'Square OAuth Secret': r'sq0cs-([0-9a-zA-Z]{18})',
    'Discord Token': r'[m|M][N|D][T|A][a-zA-Z0-9_.-]{84}',
    'Azure Storage Account Key': r'storage[account]*[\s=]*["\']*([a-zA-Z0-9+/]+={0,3})["\']*',
}



    for pattern in patterns:
                matches = re.findall(pattern, js_code)
                if matches:
                    found_data.setdefault('Custom Patterns', {}).setdefault(pattern, []).extend(matches)

            if found_data:
                sensitive_data.append({url: found_data})

        except Exception as e:
            print(f"Error analyzing JavaScript file from {url}: {str(e)}")

    return sensitive_data

    # Loop through each URL and analyze its JavaScript file
    for url in urls:
        try:
            # Use requests to get the content of the JavaScript file
            response = requests.get(url)
            js_code = response.text

            # Parse JavaScript code into AST
            js_ast = ast.parse(js_code)

            # Analyze the AST to extract sensitive information
            found_data = analyze_ast(js_ast, js_code)

            if found_data:
                sensitive_data.append({url: found_data})

        except Exception as e:
            print(f"Error analyzing JavaScript file from {url}: {str(e)}")

    return sensitive_data

def analyze_ast(node, js_code):
    found_data = {}

    for item in node.body:
        if isinstance(item, ast.Assign):
            for target in item.targets:
                if isinstance(target, ast.Attribute) and hasattr(target, 'attr'):
                    if target.attr.lower() in SENSITIVE_DATA_ATTRIBUTES:
                        if isinstance(item.value, ast.Str):
                            found_data.setdefault(target.attr, []).append(item.value.s)

        if isinstance(item, ast.FunctionDef):
            # Analyze function calls within the function
            found_data.update(analyze_function_calls(item, js_code))

    return found_data

def analyze_function_calls(node, js_code):
    found_data = {}

    for item in ast.walk(node):
        if isinstance(item, ast.Call):
            if isinstance(item.func, ast.Attribute) and hasattr(item.func, 'attr'):
                # Extract function name and arguments
                func_name = item.func.attr
                args = [astunparse.unparse(arg).strip() for arg in item.args]

                # Check for sensitive function calls
                if func_name.lower() == 'alert':
                    found_data.setdefault('Alert Calls', []).append(args)
                elif func_name.lower() == 'console.log':
                    found_data.setdefault('Console Log Calls', []).append(args)

    return found_data


def main_analysis(urls):
    sensitive_data = []

    # Loop through each URL and analyze its JavaScript file
    for url in urls:
        try:
            # Analyze JavaScript file
            result = analyze_javascript_file(url)
            sensitive_data.append(result)

        except Exception as e:
            print(f"Error in main analysis for {url}: {str(e)}")

    return sensitive_data


# Example usage with a website URL
website_urls = ["https://example1.com", "https://example2.com"]
javascript_links = [extract_javascript_links(url) for url in website_urls]

if javascript_links:
    print(f"Found JavaScript files on the websites.")

    # Analyze all JavaScript files
    result = main_analysis(javascript_links)

    if result:
        print("\nSensitive data found in the JavaScript files:")
        for data in result:
            print(data)
    else:
        print("No sensitive data found in the JavaScript files.")
else:
    print("No JavaScript files found on the websites.")